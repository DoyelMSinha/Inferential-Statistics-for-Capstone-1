# Inferential-Statistics-for-Capstone-1

Inferential Statistics : Capstone Project 1
Objective: Apply Inferential statistics techniques to explore the data
Step 1: Read CSV file with Airbnb data downloaded from http://insideairbnb.com/get-the-data.html

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
%pylab inline
df = pd.read_csv('/Users/doyelm/Documents/Project/listings.csv')
​
​
Populating the interactive namespace from numpy and matplotlib
Step2: Study the data

df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 9663 entries, 0 to 9662
Data columns (total 16 columns):
id                                9663 non-null int64
name                              9663 non-null object
host_id                           9663 non-null int64
host_name                         9661 non-null object
neighbourhood_group               0 non-null float64
neighbourhood                     9663 non-null int64
latitude                          9663 non-null float64
longitude                         9663 non-null float64
room_type                         9663 non-null object
price                             9663 non-null int64
minimum_nights                    9663 non-null int64
number_of_reviews                 9663 non-null int64
last_review                       6006 non-null object
reviews_per_month                 6007 non-null float64
calculated_host_listings_count    9663 non-null int64
availability_365                  9663 non-null int64
dtypes: float64(4), int64(8), object(4)
memory usage: 1.2+ MB

df2 = df.groupby(['neighbourhood'])[['id']].count().reset_index()
 
len(df2[df2.id > 10].sort_values('id',ascending=False ) )
41
Select top 39 neighbourhoods - from above result we see 39 neighbourhoods have sample > 10

​
df_1=df.groupby(['neighbourhood' ])[['id']].count().sort_values(by=['id'], ascending=False)[0:39]
​
# choose data for the neighbourhoods in the list created above
df_new = df[df['neighbourhood'].isin(df_1.index.values.tolist())].loc[:, ('neighbourhood', 'latitude','longitude','room_type','price','minimum_nights','number_of_reviews', 'availability_365') ]
df_new.fillna(0)   
df_new.price=df_new.price.astype(float) 
df_new.minimum_nights=df_new.minimum_nights.astype(float) 
df_new.number_of_reviews=df_new.number_of_reviews.astype(float) 
df_new.availability_365=df_new.availability_365.astype(float) 
df_new.neighbourhood=df_new.neighbourhood.astype(str) 
 
df_new.info()
 
<class 'pandas.core.frame.DataFrame'>
Int64Index: 9621 entries, 0 to 9662
Data columns (total 8 columns):
neighbourhood        9621 non-null object
latitude             9621 non-null float64
longitude            9621 non-null float64
room_type            9621 non-null object
price                9621 non-null float64
minimum_nights       9621 non-null float64
number_of_reviews    9621 non-null float64
availability_365     9621 non-null float64
dtypes: float64(6), object(2)
memory usage: 676.5+ KB
Check if data exists with price == 0

df[df.price == 0]
id	name	host_id	host_name	neighbourhood_group	neighbourhood	latitude	longitude	room_type	price	minimum_nights	number_of_reviews	last_review	reviews_per_month	calculated_host_listings_count	availability_365
last_review																
convert non-numeric binary variables into binary (0/1) variables. Converts categorical variables into dummy variables.

​
def preprocess_features(X):
    '''   converts non-numeric binary variables into
        binary (0/1) variables. Converts categorical variables into dummy variables. '''
​
    # Initialize new output DataFrame
    output = pd.DataFrame(index = X.index)
    # Investigate each feature column for the data
    for col, col_data in X.iteritems():
        #print(col,col_data)
        # If data type is non-numeric, replace all yes/no values with 1/0
        if col_data.dtype == object:
            col_data = col_data.replace(['yes', 'no'], [1, 0])
​
        # If data type is categorical, convert to dummy variables
        if col_data.dtype == object:
            # Example: 'school' => 'school_GP' and 'school_MS'
            col_data = pd.get_dummies(col_data, prefix = col)  
        
        # Collect the revised columns
        output = output.join(col_data)
    
    return output
​
#y =  log(df_new.price+1)#.fillna(0)   
y =  df_new.price
X = df_new.loc[:,df_new.columns != 'price'] 
X_all = preprocess_features(X)
 
 
​
 
Apply Inferential Statictics techniques
Split the targets into training/testing sets

from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split, GridSearchCV
df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(X_all, y, test_size=0.3, random_state=42)
Perform T test or Hypothesis testing

from scipy import stats
 
df_2=df.groupby(['room_type' ])[['id']].count().sort_values(by=['id'] , ascending=False)[0:1]
df_3=df.groupby(['room_type'])[['id']].count().sort_values(by=['id'] , ascending=False)[1:2]
#print(df_2)
#print(df_3)
df_array2=df[df['room_type'].isin(df_2.index.values.tolist())].price
df_array3=df[df['room_type'].isin(df_3.index.values.tolist())].price
​
#print(df_array1)
np.seterr(divide='ignore', invalid='ignore')
stats.ttest_ind(df_array2, df_array3, equal_var = False)
Ttest_indResult(statistic=32.78944417358676, pvalue=3.181358246813975e-222)
In the above test stats.ttest_ind is used to calculate the T-test for the means of two independent samples of scores. The test measures whether the average (expected) value differs significantly across samples. If the P-value is less than the significance level 0.05, we reject the null hypothesis.

Null hypothesis : "The average price for the apartment is not equal to the average price for the private room"


The alternatve hypothesis : " the average price is equal for different types of rooms.


As P-value is larger than 0.05 or 0.1, we reject the null hypothesis

OLS - Ordinary Least squares model

​
import statsmodels
from numpy.random import randn
import statsmodels.api as sm
​
%pylab inline
​
mymodel = sm.OLS(y,X_all)
results = mymodel.fit()
results.params
results.summary()
#results.summary2()
results_summary=results.summary2()
print(results_summary)
​
​
Populating the interactive namespace from numpy and matplotlib
                             Results: Ordinary least squares
==========================================================================================
Model:                      OLS                     Adj. R-squared:            0.140      
Dependent Variable:         price                   AIC:                       143195.8225
Date:                       2018-05-15 22:12        BIC:                       143525.7209
No. Observations:           9621                    Log-Likelihood:            -71552.    
Df Model:                   45                      F-statistic:               35.68      
Df Residuals:               9575                    Prob (F-statistic):        2.19e-282  
R-squared:                  0.144                   Scale:                     1.6957e+05 
------------------------------------------------------------------------------------------
                             Coef.      Std.Err.     t     P>|t|     [0.025       0.975]  
------------------------------------------------------------------------------------------
neighbourhood_78701         -7730.3671  2276.8559  -3.3952 0.0007  -12193.4868  -3267.2475
neighbourhood_78702         -7772.3953  2269.9319  -3.4241 0.0006  -12221.9425  -3322.8481
neighbourhood_78703         -7732.4807  2284.3869  -3.3849 0.0007  -12210.3628  -3254.5985
neighbourhood_78704         -7766.4190  2279.1774  -3.4076 0.0007  -12234.0893  -3298.7487
neighbourhood_78705         -7973.8581  2279.1357  -3.4986 0.0005  -12441.4467  -3506.2696
neighbourhood_78721         -7886.6652  2261.8632  -3.4868 0.0005  -12320.3961  -3452.9344
neighbourhood_78722         -7921.2314  2271.9686  -3.4865 0.0005  -12374.7711  -3467.6918
neighbourhood_78723         -7936.1871  2267.0820  -3.5006 0.0005  -12380.1480  -3492.2262
neighbourhood_78724         -7905.4970  2250.4770  -3.5128 0.0004  -12316.9085  -3494.0855
neighbourhood_78725         -7791.2919  2242.7681  -3.4740 0.0005  -12187.5923  -3394.9915
neighbourhood_78727         -8371.6326  2292.8001  -3.6513 0.0003  -12866.0063  -3877.2589
neighbourhood_78728         -8427.9086  2286.6614  -3.6857 0.0002  -12910.2491  -3945.5681
neighbourhood_78729         -8471.1125  2312.1853  -3.6637 0.0002  -13003.4853  -3938.7397
neighbourhood_78730         -8037.7883  2311.4373  -3.4774 0.0005  -12568.6949  -3506.8816
neighbourhood_78731         -7962.6124  2292.2558  -3.4737 0.0005  -12455.9191  -3469.3057
neighbourhood_78732         -7881.6837  2336.5741  -3.3732 0.0007  -12461.8637  -3301.5037
neighbourhood_78733         -8103.5651  2322.3559  -3.4894 0.0005  -12655.8744  -3551.2558
neighbourhood_78734         -8203.9979  2352.4929  -3.4874 0.0005  -12815.3823  -3592.6136
neighbourhood_78735         -7722.3197  2303.7151  -3.3521 0.0008  -12238.0892  -3206.5503
neighbourhood_78736         -7883.1182  2326.8949  -3.3878 0.0007  -12444.3249  -3321.9114
neighbourhood_78737         -7692.0583  2330.2941  -3.3009 0.0010  -12259.9282  -3124.1884
neighbourhood_78738         -7816.0462  2350.4120  -3.3254 0.0009  -12423.3514  -3208.7409
neighbourhood_78739         -7606.5746  2305.8673  -3.2988 0.0010  -12126.5628  -3086.5863
neighbourhood_78741         -7807.1754  2267.1100  -3.4437 0.0006  -12251.1912  -3363.1597
neighbourhood_78744         -7674.1387  2266.3510  -3.3861 0.0007  -12116.6666  -3231.6109
neighbourhood_78745         -7762.7354  2283.2620  -3.3998 0.0007  -12238.4125  -3287.0583
neighbourhood_78746         -7565.3131  2295.1324  -3.2962 0.0010  -12064.2586  -3066.3676
neighbourhood_78747         -7452.4758  2265.6805  -3.2893 0.0010  -11893.6893  -3011.2622
neighbourhood_78748         -7606.8158  2285.2550  -3.3287 0.0009  -12086.3996  -3127.2320
neighbourhood_78749         -7737.6580  2301.6597  -3.3618 0.0008  -12249.3984  -3225.9176
neighbourhood_78750         -8330.9880  2317.2952  -3.5951 0.0003  -12873.3774  -3788.5986
neighbourhood_78751         -8035.3693  2277.4489  -3.5282 0.0004  -12499.6514  -3571.0872
neighbourhood_78752         -8082.2912  2276.3430  -3.5506 0.0004  -12544.4056  -3620.1768
neighbourhood_78753         -8221.5178  2272.9742  -3.6171 0.0003  -12677.0287  -3766.0069
neighbourhood_78754         -8125.6617  2262.1859  -3.5920 0.0003  -12560.0251  -3691.2982
neighbourhood_78756         -8069.5161  2283.1003  -3.5345 0.0004  -12544.8761  -3594.1561
neighbourhood_78757         -8168.1281  2284.2169  -3.5759 0.0004  -12645.6770  -3690.5792
neighbourhood_78758         -8263.9883  2284.3110  -3.6177 0.0003  -12741.7217  -3786.2549
neighbourhood_78759         -8289.5087  2297.1493  -3.6086 0.0003  -12792.4077  -3786.6097
latitude                     2738.6133   367.5675   7.4506 0.0000    2018.1032   3459.1234
longitude                    -290.7126   303.7038  -0.9572 0.3385    -886.0363    304.6112
room_type_Entire home/apt -103095.9839 29767.3590  -3.4634 0.0005 -161446.3113 -44745.6564
room_type_Private room    -103326.5343 29767.0662  -3.4712 0.0005 -161676.2878 -44976.7807
room_type_Shared room     -103367.5741 29767.2200  -3.4725 0.0005 -161717.6291 -45017.5191
minimum_nights                 -0.5605     0.7264  -0.7717 0.4403      -1.9844      0.8633
number_of_reviews              -2.0093     0.1297 -15.4876 0.0000      -2.2636     -1.7550
availability_365                0.0649     0.0289   2.2443 0.0248       0.0082      0.1216
------------------------------------------------------------------------------------------
Omnibus:                  15169.295           Durbin-Watson:              1.955           
Prob(Omnibus):            0.000               Jarque-Bera (JB):           13620292.309    
Skew:                     9.970               Prob(JB):                   0.000           
Kurtosis:                 186.245             Condition No.:              8328717187460289
==========================================================================================
* The condition number is large (8e+15). This might indicate             strong
multicollinearity or other numerical problems.
The OLS class estimates a multi-variate regression model and provides a variety of fit-statistics.
The negative values for the constants simply means that the expected value on the dependent variable (price) will be less than 0 when all independent/predictor variables are set to 0.
Looking into the coefficients we see that room type is a significant factor followed by neighbourhood in determining the price of rooms.
linear regression - mean squared deviation - for neighbourhood and price

from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
​
​
# Create linear regression object
regr = linear_model.LinearRegression()
​
​
# Train the model using the training sets
regr.fit(df_X_train,df_y_train)
​
# Make predictions using the testing set
df_y_pred = regr.predict(df_X_test)
​
​
from sklearn.metrics import r2_score
​
print(' R-squared:',r2_score(df_y_test, df_y_pred)) 
​
​
 R-squared: 0.4312684928007976
